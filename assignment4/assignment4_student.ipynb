{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS260R Assignment4 Imitatioin Learning: Behavior Cloning & Preference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this assignment, we will focus on imitation learning & preference learning. Hence we will need some static datasets in hand, before start working on this assignment:\n",
    "\n",
    "Please install the following dependencies:\n",
    "\n",
    "```\n",
    "pip install minari[all]\n",
    "pip install gymnasium==1.0.0\n",
    "pip install mujoco==3.2.3\n",
    "pip install torchrl==0.7.0\n",
    "pip install matplotlib\n",
    "pip install tqdm\n",
    "```\n",
    "\n",
    "We tested with python==3.9.21 on Ubuntu22.04, and it should be fine for any python version >= 3.9. \n",
    "\n",
    "If you have problem installing the dependencies, try switch the python version. And if you have problem installing $\\textbf{MuJoCo}$ on MacOS or Windows, please refer to their github page https://github.com/google-deepmind/mujoco, where they provide a step-by-step instruction for building it from source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Behavior Cloning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1.1 Prepare Dataset (1pt)\n",
    "\n",
    "In this section, we will need to construct a dataset using Minari and TorchRL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minari\n",
    "import gymnasium as gym\n",
    "from torchrl.data.datasets.minari_data import MinariExperienceReplay\n",
    "from torchrl.data.replay_buffers import RandomSampler\n",
    "\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "# Load the dataset from the minari dataset to recover the properties of the dataset\n",
    "dataset_name = \"mujoco/hopper/expert-v0\"\n",
    "dataset = minari.load_dataset(dataset_name)\n",
    "\n",
    "# recover the dimension of state and action space\n",
    "state_dim = dataset.observation_space.shape[0]\n",
    "action_dim = dataset.action_space.shape[0]\n",
    "\n",
    "# recover the gymnasium environment\n",
    "env = dataset.recover_environment()\n",
    "\n",
    "# setup the parameters for the replay buffer\n",
    "batch_size = 64\n",
    "split_trajs = False\n",
    "\n",
    "\"\"\"\n",
    "Create a torchRL replay buffer from the minari dataset\n",
    "You can use whatever sampler you want, but make sure it will work for the later sections\n",
    "Please refer to the documentation: https://pytorch.org/rl/0.7/reference/generated/torchrl.data.datasets.MinariExperienceReplay.html?highlight=minari#torchrl.data.datasets.MinariExperienceReplay\n",
    "\"\"\"\n",
    "######### Your code here #########\n",
    "replay_buffer = None\n",
    "##################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Implement Policies (2pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Print out the state and action space below. \n",
    "Notice that the action space is actually bounded, which means our policy will also need to output \n",
    "\"\"\"\n",
    "print(dataset.observation_space)\n",
    "print(dataset.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set the seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# initialization function\n",
    "def weights_init_(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.orthogonal_(m.weight, gain=1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create a policy class for continuous actions as you did in Assignment 3\n",
    "The policy should take in a state, and output a vector as the action.\n",
    "Additionally, as we observed above, the action space is bounded between -1 and 1, so make sure use Tanh to restrict the output to be in this scale.\n",
    "\"\"\"\n",
    "class Policy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        activation: nn.Module=nn.ReLU,\n",
    "        hidden_dim: int=64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        ###### Your code here #######\n",
    "        \n",
    "        \n",
    "        \n",
    "        #############################\n",
    "        self.apply(weights_init_)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor):\n",
    "        return self.fc(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Implement Trainer (6pt)\n",
    "\n",
    "In this subsection, we need to build our behavior cloning trainer to train the policy. The behavior cloning loss is simply defined as the squared error between the predicted action and the expert action given the state:\n",
    "$$\n",
    "L_{BC}(D^{expert}, \\theta) = \\mathbb{E}_{(s, a) \\sim D^{expert}} \\left[ \\pi_{\\theta}(s) - a\\right]\n",
    "$$\n",
    "And we will train the policy by minimizing this loss using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following evaluation function will be helpful for you to monitor the training process and debug the code\n",
    "@torch.no_grad()\n",
    "def evaluate_policy(policy, env, num_episodes=5):\n",
    "    policy.eval()\n",
    "    rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset(seed=episode+1234)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to('cuda')\n",
    "            action = policy(state)\n",
    "            action = action.cpu().numpy()\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "            # env.render()\n",
    "            state = next_state\n",
    "        rewards.append(total_reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        replay_buffer: MinariExperienceReplay,\n",
    "        env: gym.Env,\n",
    "        lr: float = 1e-4,\n",
    "        device: str = 'cpu',\n",
    "    ):\n",
    "        # Feel free to modify this __init__ function as you needed\n",
    "        self.model = model\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.env = env\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.device = device\n",
    "\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def train_step(\n",
    "        self,\n",
    "        states: torch.Tensor,\n",
    "        actions: torch.Tensor\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Finish this function for train self.model for one step\n",
    "        * The loss should be a behavior cloning loss, i.e. MSE loss between the predicted action and the actual expert action\n",
    "        * Apply one step of gradient descent to minimize the loss\n",
    "        * The loss.item() should be returned after the gradient descent is done. \n",
    "        \"\"\"\n",
    "        \n",
    "        ###### Your code here \n",
    "        pass\n",
    "        \n",
    "        ######\n",
    "        \n",
    "    def train(self, num_steps: int, batch_size: int, eval_freq: int) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        Finish this function for training the model for num_steps steps\n",
    "        * You should sample [batch_size] data from the replay buffer\n",
    "        * You should print out the loss every [eval_freq] steps\n",
    "        \"\"\"\n",
    "        loss_log: List[float] = []\n",
    "        eval_log: List[float] = []\n",
    "        for step in tqdm(range(num_steps)):\n",
    "            \n",
    "            # Sample some data from the replay buffer\n",
    "            ##### Your code here #####\n",
    "            pass\n",
    "            ############################\n",
    "            \n",
    "            loss = self.train_step(states, actions)\n",
    "            \n",
    "            if eval_freq > 0 and step % eval_freq == 0:\n",
    "                # Do the evaluation, and append the loss as well as the evaluation to the logging list\n",
    "                ##### Your code here #####\n",
    "                pass\n",
    "                ############################\n",
    "        \n",
    "        return loss_log, eval_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Declare the device here\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Feel free to change the hyperparameters\n",
    "hidden_dim = 64\n",
    "activation = nn.ReLU\n",
    "lr = 1e-3\n",
    "\n",
    "bc_model =  Policy(\n",
    "    state_dim=state_dim, \n",
    "    action_dim=action_dim, \n",
    "    hidden_dim=hidden_dim, \n",
    "    activation=activation\n",
    ")\n",
    "\n",
    "bc_trainer = BCTrainer(\n",
    "    model=bc_model, \n",
    "    replay_buffer=replay_buffer,\n",
    "    env=env,\n",
    "    lr=lr, \n",
    "    device=device\n",
    ")\n",
    "bc_loss_log, bc_eval_log = bc_trainer.train(\n",
    "    num_steps=100000, \n",
    "    batch_size=64, \n",
    "    eval_freq=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot your results (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Preference Learning\n",
    "In this section, we will implement a preference learning algorithm. A preference learning algorithm learns not only to maximize the probability of outputing expert actions, it also learns to minimize the probability of outputing bad actions. We will leverage a \"negative dataset\", which contains poor-performing trajectories. The algorithm that you will be implementing below is largely based on https://arxiv.org/pdf/2310.13639"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Construct segment dataset (1pt)\n",
    "In this section, we will use segment datasets. The difference between a segment replay buffer and a regular replay buffer is that the segment replay buffer will sample continuous segments, whereas a regular one samples individual [state, action] pairs. Please refer to https://pytorch.org/rl/0.7/reference/generated/torchrl.data.replay_buffers.SliceSampler.html?highlight=slicesampler#torchrl.data.replay_buffers.SliceSampler for details.\n",
    "\n",
    "We need two datasets to perform preference learning: one positive dataset (pos_segment_replay_buffer) and a negative dataset (neg_segment_replay_buffer). The first one contains expert trajectories (and you will have to use proper sampler to slice them into segments), and the latter contains suboptimal trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.data.replay_buffers import SliceSampler\n",
    "\n",
    "pos_dataset_name = \"mujoco/hopper/expert-v0\" # The minari dataset contains expert trajectories\n",
    "neg_dataset_name = \"mujoco/hopper/simple-v0\" # The minari dataset contains low-reward trajecotires\n",
    "\n",
    "\"\"\"\n",
    "Construct two segment dataset below: pos_segment_replay_buffer, neg_segment_replay_buffer\n",
    "\"\"\"\n",
    "batch_size = 64\n",
    "num_segments = 8\n",
    "segment_length: int = batch_size / num_segments\n",
    "traj_key = \"episode\"\n",
    "strict_length = True\n",
    "\n",
    "##### Your code here #####\n",
    "pos_segment_replay_buffer = None\n",
    "neg_segment_replay_buffer = None\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling function\n",
    "The following function is a helper for sampling segements in your trainer, it does two things:\n",
    "1. Reshape the sampled batch data into segments of data using split_trajectories\n",
    "2. The segment sampler (SliceSampler) in torchRL sometimes gives you longer segments than you would expect. This is due to the lengths of the trajectories are not always multiple of the segment lengths. Hence, we need to make sure the sampled batch is in the right shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.collectors.utils import split_trajectories\n",
    "\n",
    "def sample_segment(slice_replay_buffer, num_segments, segment_length):\n",
    "    batch = None\n",
    "    while True:\n",
    "        batch_size = num_segments * segment_length\n",
    "        batch = slice_replay_buffer.sample(batch_size)\n",
    "        batch = split_trajectories(batch, trajectory_key=\"episode\")\n",
    "        \n",
    "        # make sure there is correct number of segements\n",
    "        if batch.shape[0] != num_segments:\n",
    "            continue\n",
    "        \n",
    "        # make sure the length of segments is correct\n",
    "        if batch.shape[1] != segment_length:\n",
    "            continue\n",
    "        \n",
    "        # make sure there is no padding in the segments\n",
    "        # NOTE: You can use masks to deal with the padding, but you will also need to adjust your learning rate\n",
    "        if batch['mask'].sum() == batch_size:\n",
    "            break\n",
    "        \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Implement Preference Learning Trainer (13pt)\n",
    "\n",
    "In this subsection, your task is to implement the preference learning trainer.\n",
    "\n",
    "Recall our behavior cloning loss is given by:\n",
    "$$\n",
    "L_{BC}(D^{expert}, \\theta) = \\mathbb{E}_{(s, a) \\sim D^{expert}} \\left[ \\pi(s; \\theta) - a\\right]\n",
    "$$\n",
    "\n",
    "In preference learning, we use $\\sigma^+$ to denote preferable segments, and $\\sigma^-$ to denote undesired ones, here, each segment can be writen as a seqence of state and actions, i.e. $\\sigma = [s_k, a_k, s_{k+1}, a_{k+1}, ...]$. \n",
    "Our preference loss is defined as:\n",
    "$$\n",
    "L_{Pref}(D^{expert}, D^{neg}, \\theta) = \\mathbb{E}_{\\sigma^+ \\sim D^{expert}, \\sigma^- \\sim D^{neg}} \\left[\n",
    "    - log \\frac{\n",
    "        e^{\\alpha \\mathbb{P}[\\sigma^+ | \\pi_{\\theta}]}\n",
    "    }{\n",
    "        e^{\\alpha \\mathbb{P}[\\sigma^+ | \\pi_{\\theta}]} + e^{\\alpha \\lambda \\mathbb{P}[\\sigma^- | \\pi_{\\theta}]}\n",
    "    }\n",
    "  \\right]\n",
    "$$\n",
    "where $\\mathbb{P}[\\sigma | \\pi_{\\theta}]$ denotes a measure of the likelihood of policy $\\pi_{\\theta}$ taking the segment $\\sigma$. And $\\alpha, \\lambda$ are hyperparameters, to control the shape of the loss.\n",
    "\n",
    "Here, we use the negative sum of squared error between the predicted action and the real action as the surrogate of this measure:\n",
    "$$\n",
    "\\mathbb{P}[\\sigma | \\pi_{\\theta}] = - \\sum_{(s_i, a_i) \\in \\sigma} \\Vert \\pi_{\\theta}(s_i) - a_i \\Vert^2\n",
    "$$\n",
    "\n",
    "Don't get confused here, the real actions are not necessarily expert actions, the real actions in $\\sigma^-$ is actually sub-optimal.\n",
    "\n",
    "As you may notice, in this loss, there are multiple of exponential and logarithmic operations, which are not numerical stable. Hence, to prevent overflow, you may need to leverage the properties of logarithm to transform this loss first, then implement it. Otherwise, you are likely to encoutner NaN in your training.\n",
    "\n",
    "To wrap up, the final loss function that you will use will be:\n",
    "\n",
    "$$\n",
    "L_{CPL}(\\theta) = L_{Perf}(D^{expert}, D^{neg}, \\theta) + \\beta L_{BC}(D^{expert}, \\theta)\n",
    "$$\n",
    "\n",
    "where the behavior cloning loss will serve as a regularizer, and $\\beta$ is the hyperparameter for controlling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastivePreferenceTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        pos_segment_buffer: MinariExperienceReplay,\n",
    "        neg_segment_buffer: MinariExperienceReplay,\n",
    "        env: gym.Env,\n",
    "        lr: float = 1e-4,\n",
    "        entropy_coeff: float = 0.01,\n",
    "        lam: float = 1e-3,\n",
    "        alpha: float = 1.0,\n",
    "        bc_loss_coeff: float = 0.1, # the coeffcient for the behavior cloning loss\n",
    "        device: str = 'cpu',\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.pos_segment_buffer = pos_segment_buffer\n",
    "        self.neg_segment_buffer = neg_segment_buffer\n",
    "        self.env = env\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.entropy_coeff = entropy_coeff\n",
    "        self.lam = lam\n",
    "        self.alpha = alpha\n",
    "        self.bc_loss_coeff = bc_loss_coeff\n",
    "        self.device = device\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def train_step(\n",
    "        self, \n",
    "        states: torch.Tensor, \n",
    "        actions: torch.Tensor, \n",
    "        neg_states: torch.Tensor,\n",
    "        neg_actions: torch.Tensor,\n",
    "    ) -> float:\n",
    "        ##### Your code here #####\n",
    "        pass\n",
    "        ##########################\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        num_segments: int,\n",
    "        segment_length: int,\n",
    "        num_steps: int = 100000,\n",
    "        eval_freq: int = 10000,\n",
    "    ):\n",
    "        loss_log = []\n",
    "        eval_log = []\n",
    "        for step in tqdm(range(num_steps)):\n",
    "            \"\"\"\n",
    "            Implement the sampling of the positive and negative segments\n",
    "            The states, actions, neg_states, neg_actions should be of shape (num_segments, segment_length, state_dim) or (num_segments, segment_length, action_dim)\n",
    "            You can use the helper function sample_segment to sample the segments\n",
    "            But you are welcome to implement your own sampling function and using masks, as long as the resulting states / actions are in the correct shape\n",
    "            \"\"\"\n",
    "            ##### Your code here #####\n",
    "            pass\n",
    "            ##########################\n",
    "            \n",
    "            # Do not modify the following assertion statements\n",
    "            assert states.shape == (num_segments, segment_length, state_dim)\n",
    "            assert actions.shape == (num_segments, segment_length, action_dim)\n",
    "            assert neg_states.shape == (num_segments, segment_length, state_dim)\n",
    "            assert neg_actions.shape == (num_segments, segment_length, action_dim)\n",
    "            \n",
    "            loss = self.train_step(states, actions, neg_states, neg_actions)\n",
    "            \n",
    "            if eval_freq > 0 and step % eval_freq == 0:\n",
    "                # Do the evaluation, and append the loss as well as the evaluation to the logging list\n",
    "                ##### Your code here #####\n",
    "                pass\n",
    "                ############################\n",
    "\n",
    "        return loss_log, eval_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a CPL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpl_model = Policy(state_dim=state_dim, action_dim=action_dim, hidden_dim=64, activation=nn.ReLU)\n",
    "cpl_trainer = ContrastivePreferenceTrainer(\n",
    "    model=cpl_model,\n",
    "    pos_segment_buffer=pos_segment_replay_buffer,\n",
    "    neg_segment_buffer=neg_segment_replay_buffer,\n",
    "    env=env, \n",
    "    lr=1e-4,\n",
    "    lam=1.0,\n",
    "    alpha=1.0,\n",
    "    bc_loss_coeff=1,\n",
    "    device='cuda',\n",
    ")\n",
    "cpl_loss_log, cpl_eval_log = cpl_trainer.train(\n",
    "    num_steps=100000, \n",
    "    num_segments=8,\n",
    "    segment_length=8,\n",
    "    eval_freq=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Investigate the effect of the hyperparaters $\\alpha, \\lambda, \\beta$ (Bonus 5pt)\n",
    "Try different choice of $\\alpha, \\lambda, \\beta$, see how does the learning curve changes, and explain why they change in those ways based on your intuition. Use your plot the explain your reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
